# Define access-restrictions for robots/spiders
# http://www.robotstxt.org/wc/norobots.html

# Some bots, like Bing's and Yahoo's, obey a Crawl-delay parameter, which
# specifies a number of seconds to wait between hits. (Bing supports only
# whole-number values.) This is a tradeoff between CPU use and search result
# freshness.
Crawl-delay: 5

# Add Googlebot-specific syntax extension to exclude forms
# that are repeated for each piece of content in the site
# the wildcard is only supported by Googlebot
# http://www.google.com/support/webmasters/bin/answer.py?answer=40367&ctx=sibling

User-agent: Googlebot
Disallow: /*sendto_form$
Disallow: /*folder_factories$
Disallow: /*?searchterm=*

# By default we allow robots to access all areas of our site
# already accessible to anonymous users

User-agent: *
Disallow: /search